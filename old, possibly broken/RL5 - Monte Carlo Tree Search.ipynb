{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Tree Search\n",
    "\n",
    "ISAE 2016 / 2017 - Tic Tac Toe Tutorial\n",
    "\n",
    "*Nicolas Schneider (email: <nls.schneider@gmail.com>)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo tree search is a decision process used in combinatorial problems. \n",
    "\n",
    "In game playing such tic tac toe, chess, go, hex … setting up an artificial intelligence player \n",
    "relies on a graph in which all different outcomes could be computed (ie: fully deterministic). \n",
    "\n",
    "Many heuristics and algorithms have been developed to provide the optimal move according to a specific game state. \n",
    "MinMax or Alpha Beta approaches show good results but the whole combinatory problem should be model.  \n",
    "\n",
    "The combinatory of some games is so huge that it becomes impossible to model all the different states.\n",
    "The combinatory for a 9x9 Go games board is approximately 5E120. \n",
    "\n",
    "Therefore heuristic approaches should be used to reduce the search space and favor accurate moves.\n",
    "\n",
    "MCTS becomes famous by beating world best players in a Go game. \n",
    "That is particularly significant as the whole AI community struggle to develop an AI player that \n",
    "could compete with amateur players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MCTS Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The basic of MCTS is simple:  \n",
    "\n",
    "   > **A search tree is built, node by node, according to the outcomes of simulated playouts**. \n",
    "\n",
    "* **Nodes** represent game states  \n",
    "* **Arcs** represent possible actions from a game state to another one. \n",
    "\n",
    "Initially, each node stores two values:  \n",
    "\n",
    "* **Play value**: that indicates how many times the node has been played.  \n",
    "* **Win value**:  that indicates how many times the node has won the game (or a quantitative payoff value).\n",
    "\n",
    "\n",
    "The process could be divided in 4 different steps: \n",
    "1. Selection\n",
    "2. Expansion\n",
    "3. Simulation\n",
    "4. Back Propagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/MCTS.png\" height=\"70%\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The whole process is repeated n times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selection process is the heart of the MCTS approach. Somtime called *Tree Policy* the selection steps has two crucial roles:\n",
    "\n",
    "> * **Select which action to play.**\n",
    "> * **Define the topology of the search space by selecting nodes to explore.**\n",
    "\n",
    "Starting from the current state **r** (depending on the game state), we select most promising nodes, until we reach a still non fully exploring node or a leaf.\n",
    "\n",
    "   *But, how to select promising nodes ?*  \n",
    "\n",
    "Each node has **play** and **win** values. These values are used to compute a **gain** for each possible action.\n",
    "\n",
    "Many strategies exist, but we will review here only the best one: **Upper Confidence Bound applied to Trees** (UCB).\n",
    "\n",
    "The UCB strategy looking to maximize the estimate rewards for each node and has the particularity to minimize your “regret” by choosing an action instead of another one. UCB formula is typically this one:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/latex"
   },
   "source": [
    "$$UCB1 = \\frac{w_i}{n_i} + \\gamma . \\sqrt{\\frac{\\ln n}{n_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:  \n",
    "* $w_i$ is the reward of node $i$ (ie. the **win** value of node $i$)  \n",
    "* $n_i$ is the number of time node $i$ was played (ie. the **play** value of node $i$)  \n",
    "* $\\gamma$ is the exploration parameter (ie. typically equal to $\\sqrt{2}$)  \n",
    "* $n$ is the overall amount of play so far (ie. the sum of play value for all sons or play value of the parent)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be noted that $\\frac{w_i}{n_i}$ encourages the exploration of higher reward choices, while $\\sqrt{\\frac{\\ln n}{n_i}}$ encourages the exploration of less visited choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The expansion step aims to select a starting point for the Monte Carlo simulation.**\n",
    "\n",
    "Once we have selected a node and if that node is not a terminal node (ie: end of the game), we select randomly a child **c**. This child **c** will be our starting point to perform a Monte Carlo simulation during the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The simulation step aims to compute a payoff.**\n",
    "\n",
    "We run a simulation from the node expand during the expansion step (**c**) until a payoff is collected (ie. or limited to a fix nuber of simulation step). \n",
    "\n",
    "The selection process during the simulation is made randomly. Once the end of the game is reached, we compute a reward depending if the player won or loose the game. With that payoff, we could move to the step 4: the back propagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The back propagation step aims to update the selected nodes of the graph build so far.**\n",
    "\n",
    "We update all visited nodes from the one we have expand in the expansion step **c** to the starting node **r** where we started the selection step. \n",
    "\n",
    "* The **play** value is increment by 1\n",
    "* The **win** value is increment by 1 (or the payoff value) for nodes that refer to the winner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MCTS do not require any expert knowledge. MCTS only need to know next states from a given condition using an action and game’s end conditions. By consequence, MCTS is generic and could be used with very minor modifications to many different games and problems.\n",
    "\n",
    "\n",
    "* MCTS builds asymmetric search tree and adapts its topology to the search space. The exploration / exploitation paradigm is directly take in account in the selection process and allows to spent time in the more relevant part of the tree.\n",
    "\n",
    "\n",
    "* MCTS is an online algorithm that provides decisions according to non-predictive user moves. \n",
    "\n",
    "\n",
    "* MCTS is adaptable and could be stop at any time. The search tree built by the algorithm could be stored and reuse later.\n",
    "\n",
    "\n",
    "* Finally, MCTS is very simple (not simplistic) and easy to code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example on Tic-Tac-Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will illustrate the MCTS algorithm on a tic tac toe board game. A 3x3 board game in which you should align 3 crosses or 3 circles to win the game.\n",
    "\n",
    "The game is not particulary complex but without optimisation (ie: symetrie, ...) we have $9!=362880$ possible sequence (*(state, action)*) and $3^9 = 19683$ possible states.\n",
    "\n",
    "Therefore, we want to demonstrate that:\n",
    "\n",
    "1. MCTS is a reinforcment learning algorithm and our AI player should learn and improve itself\n",
    "2. MCTS is a powerfull algorithm to select the best strategy without looking for all solution (without building the whole graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will implement our MCTS algorithm from a generic manner. The objective is to be able to reuse our MCTS for any board game without many modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seen above that MCTS is made of 4 functions + a 5th to made the loop\n",
    "\n",
    "1. selection\n",
    "2. expansion\n",
    "3. simulation\n",
    "4. back_propagation\n",
    "5. play\n",
    "\n",
    "\n",
    "The MCTS algorithm should have to manipulate some objects of the board game. Therefore we assess that the board game has the following functions:\n",
    "\n",
    "description | function | return\n",
    "----|----|-----\n",
    "the current player | joueur(state) | the current player (1 or 2)\n",
    "the available moves | available_move(state) | a list of actions\n",
    "the next state | next_state(state, action, current_player) | the next state according to action and depending on the current_player\n",
    "a player win | win(board, player) | true or false\n",
    "the reward | payoff(current_player, state) | the reward for the current player corresponding to the state (-1=running, 1=player1, 2=player2, 0=nul)\n",
    "the play function that actually make an action | play(state, action) | update player and board state\n",
    "\n",
    "To initiate our MCTS we need few variables:\n",
    "\n",
    "description | variable | type\n",
    "------------|----------|------\n",
    "a game | jeu | board game implementation\n",
    "who is playing | joueur | id\n",
    "the state list of the game | states | [jeu.board]\n",
    "the current state of the game| current_state | jeu.board\n",
    "the graph | graphe | {jeu.board:[]} #{key=jeu.board, value=[(action, state)]} (ie. a node with the list of sons)\n",
    "the **win** values | wins | {jeu.board:0}\n",
    "the **play** values | plays | {jeu.board:0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **selection** function should return the selected node and the list of visited nodes (we will need of that list to do the back propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **expansion** function should return the new expanded node and the update list of visited nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **simulation** do the monte carlo simulation and return a payoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **back_propagation** function do not return any object but update wins and plays values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **play** function made the loop n times and return an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T09:12:25.215964Z",
     "start_time": "2019-02-04T09:12:25.187225Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import math\n",
    "\n",
    "\n",
    "class mcts3:\n",
    "    def __init__(self, jeu, playerIA):\n",
    "        self.joueur = playerIA\n",
    "        self.jeu = jeu\n",
    "        self.states = [jeu.board]  # [état, état), ...]\n",
    "        self.current_state = jeu.board  # état\n",
    "        self.graphe = {jeu.board: []}  # {état : [(action, état), (action, état), ...]}\n",
    "        self.wins = {jeu.board: 0}  # {état : nb_de_fois_gagné}\n",
    "        self.plays = {jeu.board: 0}  # {état : nb_de_fois_visité}\n",
    "        \n",
    "    # return current_simulated_state, visited_state # s, [s, s, s, ...]\n",
    "    def selection(self, current_simulated_state, visited_state):\n",
    "        fils_nodes = self.graphe.get(current_simulated_state, 0)  # [s, s, s, ...]\n",
    "        # critère d'arret ... pas de fils. On a finit la selection, on garde l'état courant\n",
    "        while fils_nodes != 0 and len(fils_nodes) != 0:\n",
    "            visited_state.append(current_simulated_state)\n",
    "\n",
    "            # S'il pourait y avoir d'autre fils à explorer: politique d'exploration\n",
    "            moves = self.jeu.available_move(current_simulated_state)  # mouvements autorisés\n",
    "            if len(moves) != 0:  # il existe des fils non visités\n",
    "                if randint(0, 10) <= 3:  # proba 0.2 d'explorer d'autre fils\n",
    "                    # on garde le noeud courant comme selection à partir duquel on va faire l'expansion\n",
    "                    return current_simulated_state, visited_state  # s, [s, s, s, ...]\n",
    "\n",
    "            # on selectionne selon la formule UCB1 qui minimise la deception\n",
    "            len_total = math.log(sum(self.plays[s[1]] for s in fils_nodes))\n",
    "            max_value = -99999\n",
    "            best_node = ()\n",
    "            for s in fils_nodes:\n",
    "                UCB1_value = (self.wins[s[1]] / self.plays[s[1]]) + math.sqrt(2) * math.sqrt(len_total / self.plays[s[1]])\n",
    "                if UCB1_value >= max_value:\n",
    "                    max_value = UCB1_value\n",
    "                    best_node = s[1]\n",
    "\n",
    "            current_simulated_state = best_node\n",
    "            fils_nodes = self.graphe.get(current_simulated_state, 0)\n",
    "        return current_simulated_state, visited_state  # s, [s, s, s, ...]\n",
    "    \n",
    "    \n",
    "    def expension(self, current_simulated_state, visited_state):\n",
    "        current_player = self.jeu.joueur(current_simulated_state)\n",
    "        moves = self.jeu.available_move(current_simulated_state)  # mouvements autorisés\n",
    "        fils_nodes = self.graphe.get(current_simulated_state, 0)  # [(a,s), (a,s), (a,s), ...]\n",
    "        # cas d'exploration: a été choisi un noeud ou toutes les actions n'ont pas été explorées\n",
    "        if (fils_nodes != 0):\n",
    "            for fils_node in fils_nodes:\n",
    "                moves.remove(fils_node[0])  # on retire les actions déjà visitées (ie: dont le fils existe)\n",
    "        etat = current_simulated_state\n",
    "        if (len(moves) != 0):\n",
    "            # aleatoirement on choisit une action et developpons un nouvel état\n",
    "            action = moves[randint(0, len(moves) - 1)]\n",
    "            etat = self.jeu.next_state(current_simulated_state, action, current_player)\n",
    "            visited_state.append(etat)\n",
    "            # que l'on ajoute à la liste des états avec des stats egal à 0\n",
    "            self.states.append(etat)\n",
    "            # et mettons à jour le graphe\n",
    "            fils = self.graphe.get(current_simulated_state, [])\n",
    "            self.graphe[current_simulated_state] = fils + [(action, etat)]  # on enregistre le fils dans le graphe\n",
    "            self.graphe[etat] = []\n",
    "            self.wins[etat] = 0\n",
    "            self.plays[etat] = 0\n",
    "        return etat, visited_state\n",
    "    \n",
    "    \n",
    "    # return payoff\n",
    "    def simulation(self, current_simulated_state):\n",
    "        current_player = self.jeu.joueur(current_simulated_state)\n",
    "        payoff = self.jeu.payoff(current_player, current_simulated_state)\n",
    "        # Monte carlo simulation, On choisit aleatoirement des actions jusqu'au critère d'arret: fin du jeu\n",
    "        while(payoff == -1):\n",
    "            moves = self.jeu.available_move(current_simulated_state)  # mouvements autorisés\n",
    "            if (len(moves)!=0):\n",
    "                # aleatoirement on choisit une action et developpons un nouvel état\n",
    "                action = moves[randint(0, len(moves) - 1)]\n",
    "                etat = self.jeu.next_state(current_simulated_state, action, current_player)\n",
    "                # mise à jour de l'état courant\n",
    "                current_simulated_state = etat\n",
    "            current_player = self.jeu.joueur(current_simulated_state)\n",
    "            payoff = self.jeu.payoff(current_player, current_simulated_state)\n",
    "        return payoff\n",
    "    \n",
    "    \n",
    "    def back_propagation(self, visited_state, payoff):\n",
    "        for state in visited_state:\n",
    "            # mise a jour de la valeur play\n",
    "            playsState = self.plays.get(state, 0)\n",
    "            self.plays[state] = playsState + 1\n",
    "            # mise a jour de la valeur win en function du joueur\n",
    "            if self.jeu.joueur(state) != payoff and payoff != 0: # on recompense les états qui gagne\n",
    "                winsState = self.wins.get(state, 0)\n",
    "                self.wins[state] = winsState + 1\n",
    "            if self.jeu.joueur(state) == payoff and payoff != 0: # on puni les états qui perde\n",
    "                winsState = self.wins.get(state, 0)\n",
    "                self.wins[state] = winsState + 0 \n",
    "            if payoff == 0:\n",
    "                winsState = self.wins.get(state, 0)\n",
    "                self.wins[state] = winsState + 0.5\n",
    "                \n",
    "                \n",
    "    # return action\n",
    "    def play(self, current_state, n):\n",
    "        self.current_state = current_state\n",
    "        move = 0\n",
    "        # on simule n partie (ie. on construit le graphe sur n partie)\n",
    "        while (move < n):\n",
    "            current_simulated_state = current_state\n",
    "            visited_state = []\n",
    "            #current_player = self.jeu.joueur(current_state)\n",
    "            \n",
    "            #while (self.jeu.payoff(current_player, current_simulated_state) == -1):\n",
    "            current_simulated_state, visited_state = self.selection(current_simulated_state, visited_state)\n",
    "            current_simulated_state, visited_state = self.expension(current_simulated_state, visited_state)\n",
    "            payoff = self.simulation(current_simulated_state)\n",
    "            self.back_propagation(visited_state, payoff)\n",
    "            #current_player = self.jeu.joueur(current_simulated_state)\n",
    "                \n",
    "            move += 1\n",
    "        potential_play = self.graphe[current_state]\n",
    "        max_value = -99999\n",
    "        best_action = 0\n",
    "        for (a, s) in potential_play:\n",
    "            value = (self.wins[s] / self.plays[s])\n",
    "            print(\"Action:\", a, \"\\twins,plays:\", self.wins[s], \"/\", self.plays[s], \"\\tvalue: %.3f\" % value, \"\\tnb Etat:\", len(self.graphe.keys()))\n",
    "            if value >= max_value:\n",
    "                max_value = value\n",
    "                best_action = a\n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The board game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quite simple implementation. The board is a tuple of size 9 where each action refers to a position in the tuple. We store the status of the current player and who won the game.\n",
    "\n",
    "We see above the function that we have to developp ... Lets do that!!\n",
    "\n",
    "The main class and objects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* joueur(state) \n",
    "* available_move(state)\n",
    "* next_state(state, action, current_player)\n",
    "* win(board, player)\n",
    "* payoff(current_player, state) \n",
    "* play(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T09:12:31.666471Z",
     "start_time": "2019-02-04T09:12:31.643158Z"
    }
   },
   "outputs": [],
   "source": [
    "class oxo:\n",
    "    def __init__(self):\n",
    "        self.current_player = 1\n",
    "        self.actions = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "        self.board = (0,0,0,0,0,0,0,0,0)\n",
    "        self.nb_move = 0\n",
    "        self.end_game = 0 # -1: running, O: nul, 1: player 1 win, 2: player 2 win\n",
    "        \n",
    "    def joueur(self, board):\n",
    "        J1=0\n",
    "        J2=0\n",
    "        for i in board:\n",
    "            if i==1: J1+=1 \n",
    "            if i==2: J2+=1 \n",
    "        if J1==J2: return 1\n",
    "        return(2)\n",
    "    \n",
    "    def available_move(self, state):\n",
    "        am = []\n",
    "        i = 0;\n",
    "        for x in state:\n",
    "            if x == 0: \n",
    "                am += [i]\n",
    "            i += 1\n",
    "        return am\n",
    "    \n",
    "    def next_state(self, state, action, player):\n",
    "        stateList = list(state)\n",
    "        stateList[action] = player\n",
    "        return tuple(stateList)\n",
    "    \n",
    "    \n",
    "    def win(self, b, p):\n",
    "        if (b[0] == b[1] == b[2] == p or b[3] == b[4] == b[5] == p or b[6] == b[7] == b[8] == p or\n",
    "            b[0] == b[3] == b[6] == p or b[1] == b[4] == b[7] == p or b[2] == b[5] == b[8] == p or\n",
    "            b[0] == b[4] == b[8] == p or b[2] == b[4] == b[6] == p):\n",
    "            return True\n",
    "        else: return False\n",
    "        \n",
    "        \n",
    "    def asWin(self):\n",
    "        p = self.current_player\n",
    "        b = self.board\n",
    "        if (b[0] == b[1] == b[2] == p or b[3] == b[4] == b[5] == p or b[6] == b[7] == b[8] == p or\n",
    "            b[0] == b[3] == b[6] == p or b[1] == b[4] == b[7] == p or b[2] == b[5] == b[8] == p or\n",
    "            b[0] == b[4] == b[8] == p or b[2] == b[4] == b[6] == p):\n",
    "            return True\n",
    "        else: return False\n",
    "        \n",
    "    # -1: running, 0; execo, 1 joueur 1, 2 joueur 2    \n",
    "    def payoff(self, p, b):\n",
    "        nb_move = 0\n",
    "        for i in b:\n",
    "            if i != 0: nb_move += 1\n",
    "\n",
    "        if self.win(b, 1):    return 1\n",
    "        if self.win(b, 2):    return 2    \n",
    "        if nb_move == 9: return 0        \n",
    "        return -1\n",
    "    \n",
    "    \n",
    "    def play(self, state, action):\n",
    "        self.current_player = self.joueur(state)\n",
    "        stateList = list(self.board)\n",
    "        stateList[action] = self.current_player\n",
    "        self.board = tuple(stateList)\n",
    "        self.nb_move += 1\n",
    "        #print(\"nb move: \", self.nb_move)\n",
    "        self.actions.remove(action)\n",
    "        \n",
    "        \n",
    "    def myPrint(self):\n",
    "        b = []\n",
    "        for x in self.board:\n",
    "            if x == 1: b.append('X')\n",
    "            else: \n",
    "                if x == 2: b.append('O')\n",
    "                else: b.append('.')\n",
    "        print()\n",
    "        print(\"     \", b[0] , \"  \" , b[1] , \"  \" , b[2], \"       \", 0 , \"  \" , 1 , \"  \" , 2)\n",
    "        print(\"     \", b[3] , \"  \" , b[4] , \"  \" , b[5], \"  ->   \", 3 , \"  \" , 4 , \"  \" , 5)\n",
    "        print(\"     \", b[6] , \"  \" , b[7] , \"  \" , b[8], \"       \", 6 , \"  \" , 7 , \"  \" , 8)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "Finally we have to implement our **main** function to play with our MCTS Artificial player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T09:12:34.393545Z",
     "start_time": "2019-02-04T09:12:34.243210Z"
    }
   },
   "outputs": [],
   "source": [
    "import graphviz as gv\n",
    "\n",
    "playerIA = 2\n",
    "jeu = oxo()\n",
    "mcts = mcts3(jeu, playerIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T09:38:44.520103Z",
     "start_time": "2019-02-04T09:38:44.505059Z"
    }
   },
   "outputs": [],
   "source": [
    "# n est le nombre de partie faites par MCTS pour construire son arbre à chaque coup\n",
    "# IAplayer est 1 ou 2 sachant que1 commence toujours\n",
    "def jouer(n, IAplayer):\n",
    "    playerIA = IAplayer\n",
    "    jeu = oxo()\n",
    "    mcts = mcts3(jeu, playerIA)\n",
    "    loop = True\n",
    "    while (loop):\n",
    "        jeu = oxo()\n",
    "        while jeu.end_game == 0:    \n",
    "            current_state = jeu.board\n",
    "            current_player = jeu.joueur(current_state)\n",
    "            b = current_player\n",
    "            # des X et des O au lieu de 1 et 2\n",
    "            if b == 1: b='X'\n",
    "            else: b = 'O'\n",
    "        \n",
    "            if current_player == playerIA: \n",
    "                action = mcts.play(current_state, n)\n",
    "                #print(\"MCTS Action = \", action, \" from \", jeu.actions)\n",
    "            else:\n",
    "                #action = mcts.play(current_state)\n",
    "                jeu.myPrint()\n",
    "                print (\"action \", jeu.actions, \" current_player = \", b)\n",
    "                action = int(input(\"Player %s: \" % (current_player)))\n",
    "        \n",
    "            # quit the game\n",
    "            if action == 10:\n",
    "                loop=False\n",
    "                break \n",
    "            \n",
    "            # change player\n",
    "            if action == 99:\n",
    "                if playerIA==1:\n",
    "                    playerIA=2\n",
    "                else:\n",
    "                    playerIA=1\n",
    "            \n",
    "            if action in jeu.actions: \n",
    "                jeu.play(current_state, action)\n",
    "            else: \n",
    "                print (\"----- > Wrong move, try again!\")\n",
    "            \n",
    "                \n",
    "            if jeu.asWin():\n",
    "                if current_player == playerIA:\n",
    "                    print(\"------------------------------\")\n",
    "                    print(\"----------> AI WIN <----------\")\n",
    "                    print(\"------------------------------\")\n",
    "                else:\n",
    "                    print(\"------------------------------\")\n",
    "                    print(\"----------> YOU WIN <---------\")\n",
    "                    print(\"------------------------------\")\n",
    "                jeu.end_game = current_player\n",
    "        \n",
    "            if jeu.nb_move == 9 and jeu.end_game == 0:\n",
    "                print(\"------------------------------\")\n",
    "                print(\"---> No winner, No looser <---\")\n",
    "                print(\"------------------------------\")\n",
    "                jeu.end_game = -1\n",
    "    \n",
    "    # draw and export MCTS graph\n",
    "    def nb_non_zero(state):\n",
    "        i=0\n",
    "        for s in state:\n",
    "            if s!=0:\n",
    "                i=i+1\n",
    "        return i\n",
    "    \n",
    "    dot = gv.Graph(format='png')\n",
    "    i=0\n",
    "    stateDot = {}\n",
    "    for state in mcts.graphe:\n",
    "        stateDot[state]=i\n",
    "        i+=1\n",
    "    for state in mcts.graphe:\n",
    "        dot.node(str(stateDot[state]))#, str(nb_non_zero(state)))\n",
    "        for fils in mcts.graphe[state]:\n",
    "            dot.edge(str(stateDot[state]), str(stateDot[fils[1]]))#, str(fils[0]))\n",
    "    filename = dot.render(filename='g1')\n",
    "    print(filename)\n",
    "    dot.render('graphe.gv', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T09:39:06.019339Z",
     "start_time": "2019-02-04T09:38:44.905610Z"
    }
   },
   "outputs": [],
   "source": [
    "jouer(15, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## interesting behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "low n -> many mistake -> easy to observe that MCTS learn by experience\n",
    "medium n -> already powerfull, still some mistake\n",
    "high n -> always win (or nul match)\n",
    "\n",
    "asymetric search space\n",
    "able to play even for unknow state (building tree like a forest (ie. able to start from new node)\n",
    "\n",
    "inverse reward value work very well\n",
    "inverse player work well also (the tree could be used for both player)\n",
    "\n",
    "IA vs IA: converge to match nul always\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
